{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "251a37be-8ffc-4b4f-8433-27ff918ff4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import load_model\n",
    "from IPython.display import display\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb36edb-401a-4a49-af2f-5f041bf5af2d",
   "metadata": {},
   "source": [
    "Below we first start by creating some type of sequential model (which is a linear stack of different layers) -- it is commonly used in many multilayer perceptron neural networks. To create the \"Convolutional\" part of the neural network, we add the different convolutional blocks along with the RELU (Rectified Linear Unit function) and Pooling Layer to add some non-linearity. Generally, the first layer picks up some of the more basic features while the second layer picks up more complex features and this continues going on -- you pick the number of layers normally based on the task and this is a big part of model-engineering in the field of deep-learning. The non-linearity is particularly helpful becuase we don't want a model that is totally linear since that ignores a lot of the natural decision process, but furthermore, activation functions also help with the vanishing gradient problem because it will make sure small negative gradients and positive gradients do not cancel eachother out. The pooling layers, as mentioned prior, are almost exclusively because they will help with dimensionality reduction and things of the sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97f907d-ec2e-44d2-81fb-5f4cd9980b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 29, 29, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 12, 12, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 4, 4, 32)          9248      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 36,961\n",
      "Trainable params: 36,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "catdogimageclassifier = Sequential();\n",
    "\n",
    "#adding layers to the network - conv2d will add two dimensional convolutional layer which \n",
    "#have 32 filters\n",
    "catdogimageclassifier.add(Conv2D(32, (3,3), input_shape=(64,64,3))) #feature map\n",
    "catdogimageclassifier.add(Activation('relu')) #adding in relu activation\n",
    "catdogimageclassifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#adding all three convolutional blocks\n",
    "catdogimageclassifier.add(Conv2D(32, (3,3)))\n",
    "catdogimageclassifier.add(Activation('relu'))\n",
    "catdogimageclassifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "catdogimageclassifier.add(Conv2D(32, (3,3)))\n",
    "catdogimageclassifier.add(Activation('relu'))\n",
    "catdogimageclassifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "catdogimageclassifier.add(Conv2D(32, (3,3)))\n",
    "catdogimageclassifier.add(Activation('relu'))\n",
    "catdogimageclassifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#flatten the dataset which will transform the pooled feature map matrix into one column\n",
    "catdogimageclassifier.add(Flatten())\n",
    "\n",
    "#add dense function now followed by RELU activation\n",
    "catdogimageclassifier.add(Dense(64))\n",
    "catdogimageclassifier.add(Activation('relu'))\n",
    "\n",
    "#to deal with overfitting, we will use a dropout layer\n",
    "catdogimageclassifier.add(Dropout(0.5))\n",
    "\n",
    "#add one more fully connected layer to get the output in n-dimensional classes\n",
    "catdogimageclassifier.add(Dense(1))\n",
    "catdogimageclassifier.add(Activation('sigmoid'))\n",
    "catdogimageclassifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca48d02-9901-44cb-80e2-1ee44afc88e4",
   "metadata": {},
   "source": [
    "Below, the model is being compiled with a loss function (binary cross-entropy -- it's a very basic and simple technique which basically involves subtracting the differences in expected and result), a metric on how to evaluate backpropagation, and a different method of gradient descent which adjusts the learning rate. **RMSPROP**, otherwise known as *root mean squared propagation* is a technique proposed by Geoffrey Hinton that adjusts the learning rate based on certain parameters so that it isn't always constant when doing gradient descent -- note that the learning rate essentially details how \"thorough\" or \"fine-grained\" the stochastic gradient descent algorithm standardly used will go until in order to reach the minimum -- a big learning-rate will often mean that you won't necessarily be exact and might cross the minimum before returning to it while a small learning-rate takes a significant chunk of time to parse through. \n",
    "\n",
    "Let's define **target size** as the size the image is adjusted to (as in pixels) so that it can easily be fed into the Convolutional Neural Network. Let's also define the **batch-size** as the number of images that will be processed at a given time -- this is pretty much done because most machines aren't able to deal with that many images at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "091a63d2-4cae-4386-8c22-3fcfe5c5fd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 2 classes.\n",
      "Found 21000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compile the model\n",
    "catdogimageclassifier.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation to help with overfitting\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.25, zoom_range=0.25, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Loading training data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    \"/mnt/c/Users/abhi/Documents/Programs/Computer-Vision/Deep-Learning/dogsvscats/test\",\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Loading testing data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    \"/mnt/c/Users/abhi/Documents/Programs/Computer-Vision/Deep-Learning/dogsvscats/train\",\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb820f7-fb6f-4564-9306-0a9814f9edc1",
   "metadata": {},
   "source": [
    "And now here we begin the training of the model! We have 10 epochs, which are kind of like iterations of training the model and testing it, and we train over the entire training set for each epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59544360-4e67-4193-ae98-164c35fdc565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 84s 674ms/step - loss: 0.6807 - accuracy: 0.5742 - val_loss: 0.6658 - val_accuracy: 0.5897\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 82s 660ms/step - loss: 0.6656 - accuracy: 0.6087 - val_loss: 0.6375 - val_accuracy: 0.6340\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 82s 662ms/step - loss: 0.6464 - accuracy: 0.6300 - val_loss: 0.6256 - val_accuracy: 0.6636\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 80s 646ms/step - loss: 0.6279 - accuracy: 0.6490 - val_loss: 0.6038 - val_accuracy: 0.6855\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 81s 650ms/step - loss: 0.6161 - accuracy: 0.6575 - val_loss: 0.5835 - val_accuracy: 0.6977\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 84s 677ms/step - loss: 0.5947 - accuracy: 0.6908 - val_loss: 0.5824 - val_accuracy: 0.6843\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 82s 663ms/step - loss: 0.5872 - accuracy: 0.6923 - val_loss: 0.6046 - val_accuracy: 0.6631\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 81s 649ms/step - loss: 0.5675 - accuracy: 0.7117 - val_loss: 0.5722 - val_accuracy: 0.7125\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 84s 674ms/step - loss: 0.5622 - accuracy: 0.7113 - val_loss: 0.5329 - val_accuracy: 0.7393\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 80s 647ms/step - loss: 0.5440 - accuracy: 0.7265 - val_loss: 0.5467 - val_accuracy: 0.7304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f83b436aeb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#begin training now \n",
    "catdogimageclassifier.fit(\n",
    "    training_set,\n",
    "    steps_per_epoch=len(training_set),\n",
    "    epochs=10,\n",
    "    validation_data=test_set,\n",
    "    validation_steps=len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af446f00-3fad-4e53-84e7-ae14e5996dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f844d591040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "catdogimageclassifier = load_model('catdog_cnn_model.h5')\n",
    "\n",
    "from keras.preprocessing import image \n",
    "an_image = image.load_img(\"/mnt/c/Users/abhi/Documents/Programs/Computer-Vision/Deep-Learning/dogsvscats/MANUAL test/gsd.jpeg\", target_size = (64, 64))\n",
    "ar_image = image.img_to_array(an_image)\n",
    "ar_image = np.expand_dims(ar_image, axis=0)\n",
    "\n",
    "verdict = catdogimageclassifier.predict(ar_image)\n",
    "if verdict[0][0]>=.5:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print(prediction);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4babee27-5fc0-4852-a567-11a1eeccadbb",
   "metadata": {},
   "source": [
    "You can see that in the above code which is classifying an image of a german shepherd, which you can see in the folder is an actual dog!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CV)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
